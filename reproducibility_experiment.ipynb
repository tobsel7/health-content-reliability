{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249152f8",
   "metadata": {},
   "source": [
    "# 1) Initialize the project setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c93f52a",
   "metadata": {},
   "source": [
    "## 1.1) Install the python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61c4cd01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\mimi\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.5)\n",
      "Requirement already satisfied: html5lib in c:\\users\\mimi\\anaconda3\\lib\\site-packages (1.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from html5lib) (0.5.1)\n",
      "Requirement already satisfied: six>=1.9 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from html5lib) (1.16.0)\n",
      "Requirement already satisfied: pickle-mixin in c:\\users\\mimi\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Collecting argparse\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n",
      "Requirement already satisfied: numpy in c:\\users\\mimi\\anaconda3\\lib\\site-packages (1.21.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\mimi\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mimi\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: xlrd in c:\\users\\mimi\\anaconda3\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: pathlib in c:\\users\\mimi\\anaconda3\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: w3lib in c:\\users\\mimi\\anaconda3\\lib\\site-packages (1.21.0)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\users\\mimi\\anaconda3\\lib\\site-packages (from w3lib) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install nltk \n",
    "!pip install html5lib\n",
    "!pip install pickle-mixin \n",
    "!pip install argparse \n",
    "!pip install numpy \n",
    "!pip install pandas \n",
    "!pip install scikit-learn \n",
    "!pip install xlrd\n",
    "!pip install pathlib \n",
    "!pip install w3lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700e2d66",
   "metadata": {},
   "source": [
    "## 1.2) Download the necessary tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c32e707a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Mimi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mimi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the packages\n",
    "import nltk\n",
    "# download the tokenizers\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71602444",
   "metadata": {},
   "source": [
    "# 2) Run the experiment source code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b7c12",
   "metadata": {},
   "source": [
    "## 2.1) Imports, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c820456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from sklearn import preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.datasets import dump_svmlight_file\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b44b9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "z1 = 200  # empirical observed standardisation value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b355d154",
   "metadata": {},
   "source": [
    "## 2.1) Lexica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b78e5a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "lexica_files = {\n",
    "    \"privacy\": \"lexicon/privacy.txt\",\n",
    "    \"contact\": \"lexicon/contact.txt\",\n",
    "    \"stopwords\": \"lexicon/stopwords.txt\",\n",
    "    \"commercial\": \"lexicon/comm_list.txt\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2042375",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function loads a lexicon from a file\n",
    "\"\"\"\n",
    "def load_lexicon_from_file(file_path):\n",
    "    with open(file_path) as file:\n",
    "        lexicon = [word.rstrip() for word in file.readlines()]\n",
    "    return lexicon\n",
    "\n",
    "\n",
    "class Lexicon:\n",
    "    def __init__(self, file_paths):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        new_words = load_lexicon_from_file(file_paths[\"stopwords\"])\n",
    "        self.stopwords = self.stopwords.union(new_words)\n",
    "        self.commercial = load_lexicon_from_file(file_paths[\"commercial\"])\n",
    "        self.contact = load_lexicon_from_file(file_paths[\"contact\"])\n",
    "        self.privacy = load_lexicon_from_file(file_paths[\"privacy\"])\n",
    "\n",
    "\n",
    "# Load all lexica\n",
    "lexicon = Lexicon(lexica_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622feaa",
   "metadata": {},
   "source": [
    "## 2.2) Reporting functions for the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9deaf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function implements the weighted accuracy metric described in Sondhi's study\n",
    "\"\"\"\n",
    "def weighted_accuracy(bias, tn, tp, fn, fp):\n",
    "    return (bias * tp + tn) / (bias * (tp + fn) + tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aec6b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function saves in a file the obtained performance for a concrete cost-factor and feature set\n",
    "\"\"\"\n",
    "def save_results(dataset, features, cost_factor, ts, accuracies, f1_l, f1_rel_l, f1_unrel_l):\n",
    "    if not os.path.exists('./results'):\n",
    "        os.makedirs('./results')\n",
    "\n",
    "    with open(\"./results/\" + dataset + \"_results_\" + features + \"_cost_fact\" + str(cost_factor) + \"_\" + ts + \".txt\",\n",
    "              \"w+\") as f:\n",
    "        f.write(\"The mean accuracy is \" + str(np.mean(accuracies)) + \"\\n\")\n",
    "        f.write(\"The f1-score is \" + str(np.mean(f1_l)) + \"\\n\")\n",
    "        f.write(\"The credible f1-score is \" + str(np.mean(f1_rel_l)) + \"\\n\")\n",
    "        f.write(\"The non-credible f1-score is \" + str(np.mean(f1_unrel_l)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1981e817",
   "metadata": {},
   "source": [
    "## 2.3) Various feature extraction functions\n",
    "The following functions count occurrences of different types of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "277faa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function calculates the word-based features as their normalized frequency value\n",
    "\"\"\"\n",
    "def word_features(doc, vectorizer):\n",
    "    vector = vectorizer.transform([doc])\n",
    "    doc_to_list = list(vector.toarray()[0])\n",
    "    maximum = max(doc_to_list)\n",
    "\n",
    "    if maximum:\n",
    "        for val in doc_to_list:\n",
    "            index = doc_to_list.index(val)\n",
    "            doc_to_list[index] = val / maximum\n",
    "\n",
    "    return doc_to_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d17bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function counts the total commercial interest words appearances and returns the normalized frequency total value\n",
    "\"\"\"\n",
    "def count_commercial_keywords(filename, doc):\n",
    "    commercial_words = 0\n",
    "\n",
    "    with open(filename, encoding=\"utf-8\", errors=\"ignore\") as reader:\n",
    "        soup = BeautifulSoup(reader.read(), 'html5lib')\n",
    "        text = soup.get_text()\n",
    "        output = text.split(\" \")\n",
    "\n",
    "        for line in output:\n",
    "            for term in lexicon.commercial:\n",
    "                if term in line:\n",
    "                    commercial_words += 1\n",
    "\n",
    "        doc = doc.split(\" \")\n",
    "\n",
    "    return commercial_words / len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd3e5bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function counts the number of commercial links present in a webpage\n",
    "\"\"\"\n",
    "def count_commercial_links(filename):\n",
    "    with open(filename, encoding=\"utf-8\", errors=\"ignore\") as reader:\n",
    "        soup = BeautifulSoup(reader.read(), 'html5lib')\n",
    "        links = Counter([x.get('href') for x in soup.findAll('a')])\n",
    "        links = links.most_common()\n",
    "        commercial = 0\n",
    "\n",
    "        for item in links:\n",
    "            if item[0]:\n",
    "                if any(ext in item[0] for ext in lexicon.commercial):\n",
    "                    commercial += item[1]\n",
    "\n",
    "    return commercial / z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79c9afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function calculates the link-based features\n",
    "\"\"\"\n",
    "def count_links(filename):\n",
    "    with open(filename, encoding=\"utf-8\", errors=\"ignore\") as reader:\n",
    "        soup = BeautifulSoup(reader.read(), 'html5lib')\n",
    "        links = Counter([x.get('href') for x in soup.findAll('a')])\n",
    "        links = links.most_common()\n",
    "        total = 0\n",
    "        external = 0\n",
    "        contact = 0\n",
    "        privacy = 0\n",
    "\n",
    "        for item in links:\n",
    "            total += item[1]\n",
    "            if item[0]:\n",
    "                if item[0].startswith(('http', 'ftp', 'www')):\n",
    "                    external += item[1]\n",
    "                if any(ext in item[0] for ext in lexicon.contact):\n",
    "                    contact = 1\n",
    "                if any(ext in item[0] for ext in lexicon.privacy):\n",
    "                    privacy = 1\n",
    "\n",
    "        internal = total - external\n",
    "\n",
    "    return total / z1, external / z1, internal / z1, contact, privacy  # presence of contact and privacy links are boolean features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "828384e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function implements the whole casuistic of feature combinations\n",
    "\"\"\"\n",
    "def features_calc(docs, corpus, vectorizer, features):\n",
    "    for filename, doc in zip(docs, corpus):\n",
    "        doc_features = []\n",
    "\n",
    "        if features == \"link\" or features == \"comm\" or features == \"allRem\" or features == \"allKeep\":\n",
    "            links_counts = count_links(filename)\n",
    "            doc_features.extend(links_counts)\n",
    "\n",
    "        if features == \"comm\" or features == \"allRem\" or features == \"allKeep\":\n",
    "            commercial_links = count_commercial_links(filename)\n",
    "            commercial_words = count_commercial_keywords(filename, doc)\n",
    "            doc_features.extend([commercial_links, commercial_words])\n",
    "\n",
    "        if features == \"wordsRem\" or features == \"wordsKeep\" or features == \"allRem\" or features == \"allKeep\":\n",
    "            words = word_features(doc, vectorizer)\n",
    "            doc_features.extend(words)\n",
    "\n",
    "        yield doc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "615fdcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function generates the vocabulary for a given corpus\n",
    "\"\"\"\n",
    "def generate_vocabulary(corpus, min_df):\n",
    "    vectorizer = CountVectorizer(min_df=min_df)\n",
    "    vectorizer.fit(corpus)\n",
    "    return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe86c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function normalizes a text to be used as a ML algorithm input\n",
    "\"\"\"\n",
    "def __normalize_text(line, features):\n",
    "    line = re.sub('[^a-zA-Z]', ' ', line)  # remove punctuations\n",
    "    line = line.lower()  # convert to lowercase\n",
    "    line = re.sub(\"&lt;/?.*?&gt;\", \" &lt;&gt; \", line)  # remove tags\n",
    "    line = re.sub(\"(\\\\d|\\\\W)+\", \" \", line)  # remove special char and digits \n",
    "    line = line.split()  # convert string to list\n",
    "\n",
    "    if features != \"wordsKeep\" and features != \"allKeep\":\n",
    "        line = [word for word in line if not word in lexicon.stopwords]  # remove stopwords\n",
    "\n",
    "    line = \" \".join(line)\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcaba7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function extracts clean text from a given HTML file\n",
    "\"\"\"\n",
    "def preprocess_text(filename, features):\n",
    "    with open(filename, encoding=\"utf-8\", errors=\"ignore\") as reader:\n",
    "        soup = BeautifulSoup(reader.read(), 'html5lib')\n",
    "        text = soup.get_text()\n",
    "        output = text.split(\"\\n\")\n",
    "        lines = []\n",
    "\n",
    "        for line in output:\n",
    "            line = __normalize_text(line, features)\n",
    "            lines.append(line)\n",
    "\n",
    "        doc = \" \".join(lines)\n",
    "        return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b43190ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function generates an entire clean corpus from HTML files\n",
    "\"\"\"\n",
    "def generate_corpus(docs, features):\n",
    "    corpus = []\n",
    "\n",
    "    for doc in docs:\n",
    "        doc = preprocess_text(doc, features)\n",
    "        corpus.append(doc)\n",
    "\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de6fb62",
   "metadata": {},
   "source": [
    "## 2.4) Data loading functions for different data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce931bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function loads the CLEF dataset\n",
    "\"\"\"\n",
    "def data_clef():\n",
    "    if not os.path.exists('./datasets/CLEF/clef2018collection'):\n",
    "        print(\"To perform these experiments you first need to download clef2018collection\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    with open('./datasets/CLEF/CLEF2018_qtrust_20180914.txt', newline='') as assessments:\n",
    "        reader = csv.reader(assessments, delimiter=' ')\n",
    "        for row in reader:\n",
    "            web = row[2]\n",
    "            rating = int(row[3])\n",
    "\n",
    "            if rating == 0 or rating == 1 or rating == 2 or rating == 3:  # relabelling process \n",
    "                for filename in Path('./datasets/CLEF/clef2018collection').rglob(\n",
    "                        web):  # this function finds recursively a file in an entire path\n",
    "                    X.append(filename)\n",
    "                    break\n",
    "                Y.append(1)\n",
    "\n",
    "            elif rating == 7 or rating == 8 or rating == 9 or rating == 10:  # relabelling process \n",
    "                for filename in Path('./datasets/CLEF/clef2018collection').rglob(web):\n",
    "                    X.append(filename)\n",
    "                    break\n",
    "                Y.append(-1)\n",
    "\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90b841bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function loads the Schwarz dataset\n",
    "\"\"\"\n",
    "def data_schwarz():\n",
    "    df = pd.read_excel(\"./datasets/Schwarz/web_credibility_relabeled.xlsx\")\n",
    "    ratings = df['Likert Rating']\n",
    "    urls = df['URL']\n",
    "    root = os.getcwd()\n",
    "    path = './datasets/Schwarz/CachedPages'\n",
    "    os.chdir(path)\n",
    "    cached_pages_dir = os.getcwd()\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for url, rating in zip(urls, ratings):\n",
    "        try:\n",
    "            url = url.replace('http://', '')\n",
    "            url = url.split('/')\n",
    "            if url[-1]:  # this case deals with urls like 'www.adamofficial.com/us/home'\n",
    "                url = '/'.join(url[:-1])\n",
    "                os.chdir(url)\n",
    "                f = [f for f in os.listdir() if re.match(url[-1] + '*', f) and os.path.isfile(f)]\n",
    "            else:\n",
    "                url = '/'.join(url)\n",
    "                os.chdir(url)\n",
    "                f = [f for f in os.listdir() if re.match('index*', f) and os.path.isfile(f)]\n",
    "\n",
    "            X.append(os.path.join(os.getcwd(), f[0]))\n",
    "            Y.append(rating)\n",
    "            os.chdir(cached_pages_dir)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    os.chdir(root)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e1fff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function loads the Sondhi dataset\n",
    "\"\"\"\n",
    "def data_sondhi():\n",
    "    path1 = './datasets/Sondhi/reliable'\n",
    "    root = os.getcwd()\n",
    "    os.chdir(path1)\n",
    "    arr1 = os.listdir('.')\n",
    "    path2 = '../unreliable'\n",
    "    os.chdir(path2)\n",
    "    arr2 = os.listdir('.')\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for rel, unrel in zip(arr1, arr2):\n",
    "        os.chdir('../reliable')\n",
    "        X.append('./datasets/Sondhi/reliable/' + rel)\n",
    "        Y.append(-1)\n",
    "        os.chdir('../unreliable')\n",
    "        X.append('./datasets/Sondhi/unreliable/' + unrel)\n",
    "        Y.append(1)\n",
    "\n",
    "    os.chdir(root)\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e025ea",
   "metadata": {},
   "source": [
    "## 2.5) Model training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc47a7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset=\"Sondhi\", features=\"link\", dump=True, standard=True, cost_factors=[1, 2, 3], reproducibility_seed=1):\n",
    "    if dataset == \"Sondhi\":\n",
    "        X, Y = data_sondhi()\n",
    "        n = 5\n",
    "        min_df = 1\n",
    "\n",
    "    elif dataset == \"Schwarz\":\n",
    "        X, Y = data_schwarz()\n",
    "        n = 2\n",
    "        min_df = 0.5\n",
    "\n",
    "    elif dataset == \"CLEF\":\n",
    "        X, Y = data_clef()\n",
    "        n = 5\n",
    "        min_df = 0.4\n",
    "\n",
    "    else:\n",
    "        print(\"Unknown dataset\")\n",
    "        return\n",
    "\n",
    "    np.random.seed(reproducibility_seed)  # reproducibility seed\n",
    "    skf = StratifiedKFold(n_splits=n)  # stratified k-fold: preserves the percentage of samples for each class\n",
    "    ts = str(time.time())\n",
    "    print(\"EXPERIMENT ID: \", ts)  # we use the timestamp as experiment id\n",
    "\n",
    "    \"\"\"\n",
    "    For each cost-factor, we perform a n-fold cross validation for the feature set previously selected\n",
    "    \"\"\"\n",
    "    for cost_factor in cost_factors:\n",
    "\n",
    "        accuracies, f1_micro, f1_rel, f1_unrel = [], [], [], []\n",
    "        it = 1\n",
    "\n",
    "        for train_index, test_index in skf.split(X, Y):\n",
    "\n",
    "            data_train = X[train_index]\n",
    "            corpus_train = generate_corpus(data_train, features)\n",
    "            vectorizer = generate_vocabulary(corpus_train, min_df)  # for each fold we reset vocabulary associated to training set\n",
    "\n",
    "            if dump:\n",
    "                if not os.path.exists('./models'):\n",
    "                    os.makedirs('./models')\n",
    "\n",
    "                pickle.dump(vectorizer, open(\n",
    "                    f\"models/vocabulary_{dataset}_{features}_it{it}_cost_fact{cost_factor}_{ts}.pkl\", \"wb\"))\n",
    "\n",
    "            data_train = features_calc(data_train, corpus_train, vectorizer, features)\n",
    "            target_train = Y[train_index]\n",
    "\n",
    "            if standard:\n",
    "                list_data_train = list(data_train)\n",
    "                scaler_x = preprocessing.StandardScaler().fit(list_data_train)\n",
    "\n",
    "                if dump:\n",
    "                    pickle.dump(scaler_x, open(\n",
    "                        f\"models/scaler_{dataset}_{features}_it{it}_cost_fact{cost_factor}_{ts}.pkl\", \"wb\"))\n",
    "\n",
    "                data_train = scaler_x.transform(list_data_train)\n",
    "\n",
    "            elif not standard:\n",
    "                data_train = np.array(list(data_train))\n",
    "                nsamples, nx = data_train.shape\n",
    "                data_train = data_train.reshape((nsamples, nx))\n",
    "\n",
    "            if not os.path.exists('./test1'):\n",
    "                os.makedirs('./test1')\n",
    "\n",
    "            dump_svmlight_file(data_train, target_train, f\"test1/train_{ts}.txt\")\n",
    "\n",
    "            data_test = X[test_index]\n",
    "            corpus_test = generate_corpus(data_test, features)\n",
    "            data_test = features_calc(data_test, corpus_test, vectorizer, features)\n",
    "            target_test = Y[test_index]\n",
    "\n",
    "            if standard:\n",
    "                data_test = scaler_x.transform(list(data_test))\n",
    "\n",
    "            elif not standard:\n",
    "                data_test = np.array(list(data_test))\n",
    "                nsamples, nx = data_test.shape\n",
    "                data_test = data_test.reshape((nsamples, nx))\n",
    "\n",
    "            dump_svmlight_file(data_test, target_test, f\"test1/test_{ts}.txt\")\n",
    "\n",
    "            print(\"Training it=\", it, \"cost-factor=\", cost_factor)\n",
    "            clf = SVC(gamma=\"auto\", class_weight={-1:1, 1:cost_factor})\n",
    "            clf.fit(data_train, target_train)\n",
    "\n",
    "            if dump:\n",
    "                filename = f\"models/model_{dataset}_{features}_it{it}_cost_fact{cost_factor}_{ts}.dat\"\n",
    "                pickle.dump(clf, open(filename, \"wb\"))\n",
    "\n",
    "            print(\"Predicting it=\", it, \"cost-factor=\", cost_factor)\n",
    "            predictions = clf.predict(data_test)\n",
    "\n",
    "            tn, fp, fn, tp = confusion_matrix(target_test, predictions).ravel()\n",
    "\n",
    "            accuracies.append(weighted_accuracy(cost_factor, tn, tp, fn, fp) * 100)\n",
    "            f1_micro.append(f1_score(target_test, predictions, average='micro'))  # micro: calculates metrics totally by counting the total true positives, false negatives and false positives\n",
    "            cl = f1_score(target_test, predictions, average=None)  # none: returns scores for each class\n",
    "            f1_rel.append(cl[0])\n",
    "            f1_unrel.append(cl[1])\n",
    "            it += 1\n",
    "\n",
    "        print(\"The accuracy is\", np.mean(accuracies))\n",
    "        print(\"The f1-score is\", np.mean(f1_micro))\n",
    "        print(\"The credible f1-score is\", np.mean(f1_rel))\n",
    "        print(\"The non-credible f1-score is\", np.mean(f1_unrel))\n",
    "        save_results(dataset, features, cost_factor, ts, accuracies, f1_micro, f1_rel, f1_unrel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5000ae0",
   "metadata": {},
   "source": [
    "# 3) Run the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e739f91",
   "metadata": {},
   "source": [
    "## 3.1) Run the experiment with some standard settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f72fd965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT ID:  1672604653.3132997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 1 cost-factor= 1\n",
      "Predicting it= 1 cost-factor= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 2 cost-factor= 1\n",
      "Predicting it= 2 cost-factor= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 3 cost-factor= 1\n",
      "Predicting it= 3 cost-factor= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 4 cost-factor= 1\n",
      "Predicting it= 4 cost-factor= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 5 cost-factor= 1\n",
      "Predicting it= 5 cost-factor= 1\n",
      "The accuracy is 65.0\n",
      "The f1-score is 0.65\n",
      "The credible f1-score is 0.580648400667899\n",
      "The non-credible f1-score is 0.676979408804735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 1 cost-factor= 2\n",
      "Predicting it= 1 cost-factor= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 2 cost-factor= 2\n",
      "Predicting it= 2 cost-factor= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 3 cost-factor= 2\n",
      "Predicting it= 3 cost-factor= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 4 cost-factor= 2\n",
      "Predicting it= 4 cost-factor= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 5 cost-factor= 2\n",
      "Predicting it= 5 cost-factor= 2\n",
      "The accuracy is 80.0\n",
      "The f1-score is 0.736111111111111\n",
      "The credible f1-score is 0.6254554334554335\n",
      "The non-credible f1-score is 0.7916587152807718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 1 cost-factor= 3\n",
      "Predicting it= 1 cost-factor= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 2 cost-factor= 3\n",
      "Predicting it= 2 cost-factor= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 3 cost-factor= 3\n",
      "Predicting it= 3 cost-factor= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 4 cost-factor= 3\n",
      "Predicting it= 4 cost-factor= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 5 cost-factor= 3\n",
      "Predicting it= 5 cost-factor= 3\n",
      "The accuracy is 84.30555555555557\n",
      "The f1-score is 0.736111111111111\n",
      "The credible f1-score is 0.6027947727947728\n",
      "The non-credible f1-score is 0.7955684821507607\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cde937",
   "metadata": {},
   "source": [
    "# 3.2) Reproduce different experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b339368",
   "metadata": {},
   "source": [
    "## Experiment parameters\n",
    "There exist three options for the dataset:\n",
    " - CLEF\n",
    " - Sondhi\n",
    " - Schwarz\n",
    " \n",
    "Different sets of features can be used:\n",
    " - link\n",
    " - comm \n",
    " - wordsRem\n",
    " - wordsKeep\n",
    " - allRem\n",
    " - allKeep\n",
    " \n",
    "The training can be done with and without saving the models in the ./models folder:\n",
    " - True (with saving)\n",
    " - False (without saving)\n",
    " \n",
    "Standardization can be performed\n",
    " - True (with standardization)\n",
    " - False (without standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c59560cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train parameters\n",
    "dataset = \"Sondhi\"\n",
    "features = \"wordsRem\"\n",
    "dump = True \n",
    "standard = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c5b84c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPERIMENT ID:  1672605407.7864013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 1 cost-factor= 1\n",
      "Predicting it= 1 cost-factor= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 2 cost-factor= 1\n",
      "Predicting it= 2 cost-factor= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 3 cost-factor= 1\n",
      "Predicting it= 3 cost-factor= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 4 cost-factor= 1\n",
      "Predicting it= 4 cost-factor= 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 5 cost-factor= 1\n",
      "Predicting it= 5 cost-factor= 1\n",
      "The accuracy is 58.61111111111111\n",
      "The f1-score is 0.5861111111111111\n",
      "The credible f1-score is 0.29204077387143745\n",
      "The non-credible f1-score is 0.7066222833770004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 1 cost-factor= 2\n",
      "Predicting it= 1 cost-factor= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 2 cost-factor= 2\n",
      "Predicting it= 2 cost-factor= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 3 cost-factor= 2\n",
      "Predicting it= 3 cost-factor= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 4 cost-factor= 2\n",
      "Predicting it= 4 cost-factor= 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 5 cost-factor= 2\n",
      "Predicting it= 5 cost-factor= 2\n",
      "The accuracy is 72.5925925925926\n",
      "The f1-score is 0.5888888888888888\n",
      "The credible f1-score is 0.29377990430622003\n",
      "The non-credible f1-score is 0.709219685974403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 1 cost-factor= 3\n",
      "Predicting it= 1 cost-factor= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 2 cost-factor= 3\n",
      "Predicting it= 2 cost-factor= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 3 cost-factor= 3\n",
      "Predicting it= 3 cost-factor= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 4 cost-factor= 3\n",
      "Predicting it= 4 cost-factor= 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Mimi\\anaconda3\\lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training it= 5 cost-factor= 3\n",
      "Predicting it= 5 cost-factor= 3\n",
      "The accuracy is 79.44444444444444\n",
      "The f1-score is 0.5888888888888888\n",
      "The credible f1-score is 0.29377990430622003\n",
      "The non-credible f1-score is 0.709219685974403\n"
     ]
    }
   ],
   "source": [
    "# start of model training\n",
    "train(dataset, features, dump, standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fc347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
